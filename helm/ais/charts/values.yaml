#
# Default values.yaml for AIS deployment chart.
#
# A new deployment *will* have to modify some of these defaults - we can't know intended
# disk paths etc in advance.
#
# If using helm directly such over-rides may be best captured in a wrapper script to
# 'helm install' using --set and --set-string.
#
# A cleaner approach is to use the likes of ArgoCD and edit this values.yaml file
# as needed, managing changes in Git and applying them with ArgoCD.
#
# No support for Kustomize at this time.
#
# The values that likely will require attention in a new deployment are grouped
# together under 'aiscluster'. Others are less likely to need tweaking (and should
# probably be removed from this file and instead tweaked via Kustomize when needed).
#
# You'll also need to visit the monitoring section - tags, graphite, grafana,
# prometheus, external_monitoring.
#
# Instead of leaving blank values, the values in sections for 'aiscluster' and for
# monitoring (all preceding the big banner announcing the end of likely changes)
# are provided for a configuration as follows:
#
# - using ArgoCD for deployment, editing this value file and committing in Git
#   and driving updates from that
# - the application name in ArgoCD (release name in Helm) is 'foo'; the chart
#   and values don't ever mention that directly, but we need it in labeling
#   examples below
# - we'll label 20 nodes as target nodes (nvidia.com/ais-target=foo-ais)
#   [aiscluster.expected_target_nodes]; if we don't want to pre-declare the
#   intended number then set this to 0
# - we'll label those same 20 nodes to run electable proxies (eligible to be
#   primary proxy at any time, nvidia.com/ais-proxy=foo-ais-electable)
# - AIS cluster state (cached cluster map etc) will be stored under /etc/ais
#   on each node, exposed as a hostPath mount [aiscluster.hostpathPrefix]
# - Container images are sourced from private repos on quay.io using a pre-installed
#   secret named 'ais-pull-secret' which exist in the same namespace we intend to
#   deploy AIStore to [aiscluster.image.*]
# - We will configure an Ingress with externalIP 10.150.172.62 (satisfied by metallb
#   in our case) and external port 51080. This is where external clients will contact
#   the AIStore gateway/proxy service. We direct the traffic to the same port number
#   internally [aiscluster.ingress.gateway]
# - Creating that external ingress will also cause us to use a hostPort on target pods 
#   using host port 51081; if not creating an external ingress set this to 0 to avoid
#   opening a host port to the target pod
# - The k8s cluster was created with a CIDR of 192.168.0.0/18; AIStore will recognize
#   client source IPs outside this range as being external to the cluster, and proxy
#   redirects to targets will be to the hostIP:hostPort (internal clients, if any,
#   are directed to the podIP:podPort) [aiscluster.k8s.cluster_cidr]
# - All target nodes have 10 x distinct filesystems for our use, all mounted at
#   /ais/sd[a-j] on each node [aiscluster.target.mountPaths]
# - We will deploy internal monitoring - Graphite, Grafana, Prometheus
# - the chart will create a PV/PVC pair for both Graphite and Grafana, both using local
#   storage on target node 'aistore0756'
#

aiscluster:
  #
  # Expected number of target pods in deployment. Set to 0 to leave initial
  # cluster deployment to work this out for itself, but initial deployment
  # is smoother if you provide this guide.
  #
  expected_target_nodes: 20
  #
  # Prefix used on aisnode host nodes for local storage for AIS cluster state.
  #
  hostpathPrefix: "/etc/ais"
  #
  # Container images, pull secret names, pull policy.
  #
  image:
    aisnode:
      repository: "quay.io/nvidia/aisnode"
      tag: "20200429.1"
    kubectl:
      repository: quay.io/nvidia/ais-kubectl
      tag: 1
    pullPolicy: IfNotPresent
    pullSecretNames:
      # k8s secret name to use in pulling images; secret populated manually
      - ais-pull-secret
  #
  # Ingress to the proxy/gateway service and to any Grafana instance
  #
  ingress:
    gateway:
      externalIP: "10.150.172.62"        # metallb external IP for ingress
      port:       51080                  # external ingress port
      targetPort: 51080                  # port used within pod
    grafana:
      externalIP: ""                     # metallb external IP for Grafana instance
      port:       3000                   # external ingress port
      targetPort: 3000                   # port used within pod
  #
  # If using external ingress into the AIS cluster then cluster_cidr must be
  # set to the pod CIDR range in use on k8s covering *all nodes* (ie not just
  # the podCIDR from any one node). If using Kubespray to deploy k8s this is
  # kube_pods_subnet in group_vars/k8s-cluster/k8s-cluster.yml.
  #
  # In playbooks we change kubelet to permit the unsafe somaxconn sysctl,
  # and we apply to aisnode pods with the value below.
  #
  k8s:
    cluster_cidr:           "192.168.0.0/18"    # must be set when using ingress with metlalb
    container_capabilities:
      # Needed for debug if you wish to run delve within a pod (or look into kubesquash etc)
      #- SYS_PTRACE
    sysctls:
      somaxconn: 100000
  #
  # Mount paths on each target node to be used for AIS bucket data. These should be
  # precreated filesystems, and no two paths must reside in the same filesystem.
  #
  target:
    hostPort: 51081
    mountPaths:
      - /ais/sda
      - /ais/sdb
      - /ais/sdc
      - /ais/sdd
      - /ais/sde
      - /ais/sdf
      - /ais/sdg
      - /ais/sdh
      - /ais/sdi
      - /ais/sdj

#
# If you have an existing graphite installation then set the builtin_monitoring
# tag to false and supply the host (or IP) and port for graphite in
# map external_monitoring.
#
# If builtin_monitoring is true and you don't want this chart to install
# Prometheus then set the prometheus tag to false.
#
tags:
  builtin_monitoring: true
  prometheus:         true

#
# Alternatively, leave the builtin-monitoring tag true (the default) and
# we'll use subchart dependencies to deploy graphite and grafana within the k8s
# cluster.
#
# If data persistence is enabled for Graphite and Grafana then local storage
# must already have been assigned on the indicated node and path combinations
# below - we don't create the underlying storage here, we're just creating a PV
# from existing local storage to satisfy the PVC made from graphite and grafana.
#
# XXX TODO:
#
#   - would be nice to add some standard dashboards; do this via another sidecar
#

#
# Key paths here that match those of https://github.com/kiwigrid/helm-charts/tree/master/charts/graphite
# will over-ride default values in the graphite dependency. Local additions are all within the ais map.
#
# If 'persistence' is set to true then we require a PV/PVC. There are two choices:
#
# 1. Create a PV and PVC externally (i.e. outside of this chart) and simply quote the PVC
#    under 'existingClaim' below. Use whatever storage class etc you want. Leave ais.pv.path
#    as the empty string.
#
# 2. To have this chart create a simple local-storage PV at a specified path on a specified node
#    an a corresponding PVC leave the existingClaim as "ais-graphite-pvc" and complete the three
#    items under ais.pv.  NOTE: You cannot update these values once the PV/PVC pair is created.
#
graphite:
  persistence:
    enabled:       true
    existingClaim: ais-graphite-pvc
  ais:
    pv:
      capacity: "500Gi"
      path:     "/stats/graphite"
      node:     "aistore0756"

#
# Key paths here that match those of https://github.com/helm/charts/tree/master/stable/grafana
# will over-ride default values in the grafana dependency. Local additions are all within the ais map.
#
# See notes for graphite about regarding selecting storage for persistence.
#
grafana:
  persistence:
    enabled:       true
    existingClaim: ais-grafana-pvc
  ais:
    pv:
      capacity: "50Gi"
      path:     "/stats/grafana"
      node:     "aistore0756"
  service:
    type: NodePort
  sidecar:
    datasources:
      enabled: true
      label:   ais_grafana_datasource
    dashboards:
      enabled: false
      label:   ais_grafana_dashboard

#
# Key paths here that match those of https://github.com/helm/charts/tree/master/stable/prometheus
# will over-ride default values in the grafana dependency. Local additions are all within the ais map.
#
# XXX TODO enable persistence
#
prometheus:
  alertmanager:
    persistentVolume:
      enabled: false
  server:
    persistentVolume:
      enabled: false

#
# Used only if builtin_monitoring is over-ridden to false. No Grafana or Prometheus here - we
# just arrange to send AIS stats to Graphite, and the external provider is responsible for
# node metrics, visualization etc.
#
external_monitoring:
  graphite_host: somehost
  graphite_port: 2003

# ---------------------------------------------------------------------------------
# |                                                                               |
# |          Values below much less likely to need changing                       |
# |                                                                               |
# ---------------------------------------------------------------------------------

#
# "Common" ais.json config values - actually not genuinely common and applicable to
# all of electable/nonelectable/target, but since those templates have duplicated
# sections we may as well avoid also duplicating values.
#
# Don't add anything here that does not contribute to ais.json; and only add
# parametrized elements to _ais_common.json - no literal values there.
#
ais_config_common:
  auth:
    secret:  NotInUse
    enabled: false
    creddir: ""
  checksum:
    type: xxhash
    validate_cold_get: true
    validate_warm_get: false
    validate_obj_move: false
    enable_read_range: false
  client:
    # test value
    client_timeout:      120s
    client_long_timeout: 30m
    list_timeout:        10m
  cloud_provider: ""
  compression:
    block_size: 262144
    checksum: false
  disk:
    disk_util_low_wm:  20
    disk_util_high_wm: 80
    disk_util_max_wm:  95
    iostat_time_long:  2s
    iostat_time_short: 100ms
  distributed_sort:
    compression:           never
    duplicated_records:    ignore
    missing_shards:        ignore
    ekm_malformed_line:    abort
    ekm_missing_key:       abort
    default_max_mem_usage: 80%
    dsorter_mem_threshold: 100GB
    call_timeout:          10m
  downloader:
    timeout: 1h
  ec:
    enabled:       false
    objsize_limit: 262144
    data_slices:   2
    parity_slices: 2
    batch_size:    64
    compression:   never
  fshc:
    enabled:       true
    test_files:    4
    error_limit:   2
  keepalivetracker:
    proxy:
      interval: 10s
      name:     heartbeat
      factor:   3
    target:
      interval: 10s
      name:     heartbeat
      factor:   3
    retry_factor: 5
    timeout_factor: 3
  log:
    dir:       /var/log/ais
    level:     3
    max_size:  4194304
    max_total: 67108864
  lru:
    lowwm:             75
    highwm:            90
    out_of_space:      95
    dont_evict_time:   120m
    capacity_upd_time: 10m
    enabled:           true
  mirror:
    copies:            2
    burst_buffer:      512
    util_thresh:       0
    optimize_put:      false
    enabled:           false
  periodic:
    stats_time:      10s
    retry_sync_time: 2s
  rebalance:
    enabled:         true
    compression:     never
    dest_retry_time: 2m
    quiescent:       20s
    multiplier:      4
  timeout:
    max_keepalive:        4s
    cplane_operation:     2s
    send_file_time:       5m
    startup_time:         1m
    max_host_busy:        1m
  versioning:
    enabled:           true
    validate_warm_get: false


proxy:
  name: proxy        # A component label for selector
  config:
    proxy:
      non_electable: false
    test_fspaths:
      count:    0
      instance: 0
    net:
      ipv4:               ""
      ipv4_intra_control: ""
      ipv4_intra_data:    ""
      l4:
        port:               51080
        port_intra_control: ""
        port_intra_data:    ""
        sndrcv_buf_size:    131072
      http:
        write_buffer_size: 0
        read_buffer_size:  0
        use_https:        false
        chunked_transfer: true
  service:
    type: ClusterIP
    port: 51080
  nodeSelector:
    key: nvidia.com/ais-proxy
  resources: {}
  # Apply the below node label on any node (just 1), the proxy runs on that node will become a primary at launch
  initialPrimaryProxyNodeLabel:
    name:  "nvidia.com/ais-initial-primary-proxy"

ne_proxy:
  name: "ne_proxy"    # A component label for selector
  config:
    proxy:
      non_electable: true
    test_fspaths:
      count:    0
      instance: 0
    net:
      ipv4:               ""
      ipv4_intra_control: ""
      ipv4_intra_data:    ""
      l4:
        port:               51080
        port_intra_control: ""
        port_intra_data:    ""
        sndrcv_buf_size:    131072
      http:
        write_buffer_size: 0
        read_buffer_size:  0
        use_https:        false
        chunked_transfer: true
  service:
    type: ClusterIP
    port: 51080      # must match that of proxy since they go behind same clusterIP service
  nodeSelector:
    key:   nvidia.com/ais-proxy
  resources: {}

target:
  name : "target"   # A component label for selector
  config:
    proxy:
      non_electable: false
    test_fspaths:
      count:    0
      instance: 0
    nodiskio:
      enabled:    false
      dryobjsize: "8M"
    net:
      ipv4:               ""
      ipv4_intra_control: ""
      ipv4_intra_data:    ""
      l4:
        port:               51081
        port_intra_control: ""
        port_intra_data:    ""
        sndrcv_buf_size:    0
      http:
        write_buffer_size: 65536
        read_buffer_size:  65536
        use_https:         false
        chunked_transfer:  true
  service:
    port:     51081
  nodeSelector:
    key:   nvidia.com/ais-target
  resources: {}

#
# Note that out target/proxy/ne_proxy DaemonSets use any resource values from their
# respective values sections.
#
resources: {}

tolerations: {}

# Make sure the DFC target only deploy to the node that are marked with a label that signify a hi-perf
# storage
# target-node-key-name : target-node
affinity: {}
#affinity:
#  requiredDuringSchedulingIgnoredDuringExecution:
#    nodeSelectorTerms:
#    - matchExpressions:
#        - key: beta.kubernetes.io/instance-type
#          operator: In
#          values:
#            - d1.8xlarge
