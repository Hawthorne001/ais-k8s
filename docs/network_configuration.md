
### Network configuration:
   1. **Network Interface Settings**:
      - Enable Jumbo Frames, which allows for larger packet sizes, reducing overhead and improving performance.
      - Enable Multiqueue as it helps in distributing network traffic processing across multiple CPU cores, enhancing network throughput and performance.
        - The `ethtool` command can be used for this purpose. Instructions are detailed in the [ais_enable_multiqueue playbook](../playbooks/host-config/docs/ais_enable_multiqueue.md).
      - See the [AIS performance doc](https://github.com/NVIDIA/aistore/blob/main/docs/performance.md#network) for more info on network settings.

   2. **Intra-cluster and Internet Connectivity**:
      - Consider using `iperf` to test connectivity between nodes to ensure robust network performance before deployment.
      - AIS uses [K8s cluster networking](https://kubernetes.io/docs/concepts/services-networking/#the-kubernetes-network-model) for intra-cluster communication between proxies and targets.
      - No specific CNI is required, but we have tested with Calico and Flannel. 
      - AIS requires public network access for integration with cloud-based storage backends and optionally container image pulls.
        - Confirm proper internet connectivity and DNS resolution from your K8s nodes (e.g., test with `ping github.com`).

   3. **Port Configuration**:
      AIS requires ingress ports, which can be configured in the cluster resource spec. The default values are listed below. 
      - **General Requirement**:
        - Open ports `51080` and `51081` for ingress traffic to allow external access to the AIS cluster.
      - **ProxySpec Default Settings**:
        - `servicePort`: `51080` (used for general service access)
        - `portPublic`: `51080` (public-facing port for external communication)
        - `portIntraControl`: `51082` (internal control communication within the cluster)
        - `portIntraData`: `51083` (data transfer within the cluster)
      - **TargetSpec Default Settings**:
        - `servicePort`: `51081` (service port dedicated for target nodes)
        - `portPublic`: `51081` (public-facing port for target nodes)
        - `portIntraControl`: `51082` (control port for internal communication among target nodes)
        - `portIntraData`: `51083` (data transfer port among target nodes)
      - **ServiceAuth**:
        - Port `52001` is used for service authentication purposes.

### Optimizing Network Bandwidth

#### Matching Bandwidth with Drive Throughput

- The network bandwidth on target nodes should ideally be proportional to the cumulative throughput of all drives. This ensures that the network can handle the data traffic generated by the drives without bottlenecks.

- With smaller datasets that can be cached primarily in memory, it can be beneficial to have higher network bandwidth than the aggregated disk throughput to accommodate potential data traffic spikes.

#### Proxy Pods and Network Traffic

- **Traffic Characteristics**: Proxy pods primarily handle small packet traffic, which includes HTTP requests and redirects.
However, the total volume of this traffic is relatively low.

- **Co-Location with Target Nodes**: Running Proxy pods on the same nodes as target nodes typically doesn't lead to network congestion, due to the modest volume of traffic generated by these pods.

#### Network Configuration for Pods

- **Single Interface Configuration**: In our setup, pods are not configured with multiple interfaces.
This means that both intra-cluster traffic (like data rebalancing) and external traffic are handled over the same network interfaces.